---
title: "R Rated Final Project - Brazilian Houses"
author: "Janhvi Goje, Ayham Alroumi, Davoud Danish"
date: "2024-05-07"
output:
  pdf_document: 
      latex_engine: xelatex
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,echo = FALSE,warning=FALSE,message=FALSE}
# Importing the libraries needed 
library(gam)
library(xgboost)
library(tidyverse)
library(glmnet)
library(caret)
library(ggplot2)
library(ggrepel)
library(ggpubr)
library(dplyr)
library(plyr)
library(maps)
library(cowplot)
library(MASS)
library(rpart)
library(rpart.plot)
library(gglasso)
library(factoextra)
library(cluster)
library(corrplot)
library(formattable)
library(factoextra)

```

```{r,echo = FALSE,warning=FALSE,message=FALSE}
# Importing the dataset
Data = read.csv("/Users/janvigoje/Desktop/Projec/BrazHousesRent.csv", sep = ",", dec = ".", header = T, colClasses = "character")
```


### Purpose

We pick the 'Brazilian Houses' dataset.

The goal of this project is to conduct a thorough data-driven analysis of the rent prices in different Brazilian cities in order to help a new company understand what kind of houses grant the larger (rent) revenue before investing themselves, in particular focusing on determining characteristics that correlate with higher rent prices. Additionally, we seek to explore whether rental markets can be segmented into distinct groups based on geographical location.

```{r,warning=FALSE,echo=FALSE,results='hide'}
head(Data)
```

### Description of the Dataset

This dataset encompasses 10,962 rental properties across various Brazilian cities and includes 12 distinct features, aimed at providing insights into the house rental market in some of Brazil's key cities. 

Five of these variables are continuous: fire insurance, property tax, HOA fee, rent amount and area; while the remianing three are categorical: animal accepted,apartment furnished, and city (where property is located). 

### Data Cleaning

Let's start of by cleaning the dataset by looking at any NAs or duplicates present.

```{r}
anyDuplicated(Data)
anyNA(Data)
```

We notice that there are duplicates and no NAs, therefore we proceed by removing the duplicates.

```{r}
Data <- unique(Data)
```

 However, we do notice that in spite of no missing data in the form of 'NA' is present, there are '-' present in floor. Let's check how many of these values are present.
 
```{r,warning=FALSE,echo=FALSE}
count<- sum(Data$floor == "-")
print(paste("Count for floor = '-':", count))
```
 
We have 2369 observations with floor '-' in our dataset. This might suggest that the '-' symbol likely represents properties located on the ground floor or standalone houses that are not part of a multi-story condominium. If the later were to be true afterall, it makes sense for the 'HOA' variable (Monthly Homeowners Association Tax) to not exist since it is a measure of a fee paid by residents in order to cover the cost of maintaining common areas, building amenities, and utilities. Let's check this in order to verify if the assumption we're making is appropriate or not.

```{r,warning=FALSE,echo=FALSE}
groundFloorHouses <- subset(Data, floor == "-", select = hoa..R..)
groundFloorHouses[,1] = as.numeric(groundFloorHouses[,1])

countHOAzero <- sum(groundFloorHouses[, 1] == 0)

print(paste("Number of observations with floor = '-' and HOA = 0:", countHOAzero))
proportionHOAzero <- countHOAzero / length(groundFloorHouses[, 1])
print(paste("Proportion of HOA = 0:", proportionHOAzero*100,"%"))

```

Around 85% of the '-' observations have HOA as 0 which goes to show that our assumption is valid and therefore, it is appropriate to replace '-' with 0 in floor.

```{r}
Data[Data == "-"] <- 0
```

Lets now categorise variables as numeric, for continous and factor, for categorical.

```{r,warning=FALSE,echo=FALSE,message=FALSE,results='hide'}
categorical <- c("city","animal","furniture")
allcols <- colnames(Data)
for(i in 1:ncol(Data)){
  if (allcols[i] %in% categorical){
    Data[,i] = as.factor(Data[,i])
  }else{
    Data[,i] = as.numeric(Data[,i])   #this means they are continuous
  }
}

str(Data)
```

Since we have the information on cities, it might be helpful to visualise the data in the form of a map. We therefore calculate the average rent prices as per the cities and visualise them using 'map_plot' function of 'ggplplot2'.

```{r,warning=FALSE,echo=FALSE}
avgrentp <- function(City) {
  cityrent <- subset(Data, city == City, select = rent.amount..R..)
  avgrent <- mean(cityrent$rent.amount..R..)
  return(avgrent)
}
```

```{r fig2, fig.height = 4, fig.width = 6, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
city_data <- data.frame(
  City = c("Belo Horizonte", "Campinas", "Porto Alegre", "Rio de Janeiro", "São Paulo"),
  Latitude = c(-19.9167, -22.9071, -30.0346, -22.9068, -23.5505),
  Longitude = c(-43.9345, -47.0632, -51.2177, -43.1729, -46.6333),
  Average_Rent = sapply(c("Belo Horizonte", "Campinas", "Porto Alegre", "Rio de Janeiro", "São Paulo"), 
                        function(x) avgrentp(x))
)

city_data$Alpha <- (city_data$Average_Rent -
                      min(city_data$Average_Rent)) / (max(city_data$Average_Rent) 
                                                      - min(city_data$Average_Rent))

map <- map_data("world", region = "Brazil")

ggplot() +
  geom_polygon(data = map, aes(x = long, y = lat, group = group), fill = "lightgray", color = "white") +
  geom_label_repel(data = city_data, aes(x = Longitude, y = Latitude, label = City, fill = Average_Rent), color = "black", size = 3,
                   box.padding = 0.5, point.padding = 0.2, force = 1, segment.color = "transparent") +
  geom_point(data = city_data, aes(x = Longitude, y = Latitude), alpha = 0.8, size = 5) +
  labs(title = "Average Rents - Brazil",
       x = "Longitude",
       y = "Latitude") +
  scale_fill_gradient(low = "green", high = "red") +
  theme_minimal()

```

São Paulo seems to have the most expensive average rent prices as we can see.  Belo Horizonte is next, followed by Rio de Janeiro, having comparatively lower rental prices while Campinas and Porto Alegre have the lowest rental prices.

Further, let's look at the correlations between certain factors 

```{r,warning=FALSE,echo=FALSE,message=FALSE,results='hide'}
correlations_with_rent <- cor(Data[, sapply(Data, is.numeric)], Data$rent.amount..R..)
correlations_with_rent
```
```{r}
cor(Data$fire.insurance..R..,Data$rent.amount..R..)
```

We notice that fire insurance has the highest correlation with the rent prices, indicating that fire insurance could be a significant variable for predictive modeling.

## Task 1

The objective of our first task is to  *Build a predictive model and find out the rent amount according to the house specifics*, by using regression methods on our response variable: *rent.amount..R..*

### Lower Dimensional Models

Let's delve into the relationship of our response variable with other variables in order understand which variables are important for our case. First, we use a Log-Linear regression model to measure the percentage increase when there is a unit increase in area.

```{r,warning=FALSE,echo=FALSE}
fit1 <- lm(log(rent.amount..R..) ~ area,data = Data)
summary(fit1)$coefficients
```

We see that there is a positive coefficient for area, suggesting a positive relationship between the area of a property and its rent price; as the area increases, the rent price also tends to increase. Given the logarithmic transformation of the rent, a coefficient of 0.0002528586 for the area implies a multiplicative effect on the original rent scale, i.e. approximately a 0.025% increase in rent for a one-unit increase in area.

Now, let's put ourselves in the perspective of the people wanting to rent a place in Brazil through the construction of a Decision Tree. This is integral in understanding the factors that influence a persons decision when selecting a house to rent.

```{r fig3, fig.height = 3, fig.width = 4, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
dtree <- rpart(rent.amount..R.. ~ rooms + furniture + bathroom, data = Data, method = "anova")
rpart.plot(dtree)
```

This decision tree helps in understanding which factors significantly influence rental prices. Properties with more rooms and additional bathrooms generally command higher rents.The unfurnished status, particularly in properties with fewer rooms and bathrooms, often correlates with lower rent, suggesting that smaller, unfurnished properties are on the cheaper end of the rental market spectrum.

Following the right-most branches tends to associate with higher rent. This is depicted by properties having 4 or more rooms and possibly other features not fully visible in the cut-off image. Conversely, properties with less than 4 rooms and possibly fewer bathrooms or being not furnished are associated with cheaper rent, as seen in the left-most branches.

### Data Pre-Processing

We begin by addressing outliers in our dataset, starting with the continuous variables, which we'll handle using the Interquartile Range (IQR) method. For discrete numerical variables, we will visually inspect boxplots to identify any outliers. This step allows us to manually scrutinize and decide on the removal of data points that appear significantly distant from the rest of the data distribution.

```{r,warning=FALSE,echo=FALSE,results='hide'}
cont <- c("area","fire.insurance..R..","property.tax..R..","hoa..R..", "floor")

#Removing outliers of continuous variables using IQR
for(i in 1:ncol(Data)){
  if (allcols[i] %in% cont){
    Q3 <- quantile(Data[,i], .75)
    IQR <- IQR(Data[,i])
    Data <- subset(Data, Data[,i]< (Q3 + 3.5*IQR))
  }
}

# Boxplots for discrete variables
'discrete <- c("rooms", "floor", "parking.spaces", "bathroom")
for(i in 1:length(discrete)){
 boxplot(Data[, discrete[i]], xlab = discrete[i])
}'

```

```{r,warning=FALSE,echo=FALSE}
#Removing outliers from discrete variables
Data <- subset(Data, floor < 40)
Data <- subset(Data, rooms < 10)
Data <- subset(Data, bathroom < 7)
```

We proceed by encoding the categorical variables 'animal' and 'furniture' into binary numerical format. In the original dataset, these variables are represented as character strings, such as "accept" and "not accept" for 'animal'. To streamline data processing and facilitate statistical analysis, we convert these categories into 0s and 1s. 

```{r,warning=FALSE,echo=FALSE}
temp <- Data$animal
temp <- as.character(temp)
temp[temp == "acept"] <- 1      
temp[temp == "not acept"] <- 0       
temp <- as.factor(temp)
Data$animal <- temp

temp <- Data$furniture
temp <- as.character(temp)
temp[temp == "furnished"] <- 1     
temp[temp == "not furnished"] <- 0      
temp <- as.factor(temp)
Data$furniture <- temp
```

We divide the dataset into training, validation, and test subsets. We standardize these subsets using the mean and standard deviation calculated from the training set, ensuring that our model generalizes well on unseen data by using the same scale across all subsets. Additionally, we combine the training and validation sets into a primary training dataset, which we will later use for final model assessments and predictions on the test set.

```{r,warning=FALSE,echo=FALSE}
set.seed(1)
trainrows <- createDataPartition(Data$rent.amount..R.., p=0.8, list=FALSE)
training_set <- Data[trainrows,]
d_test <- Data[-trainrows,]
trainrows <- createDataPartition(training_set$rent.amount..R.., p=0.8, list=FALSE)
d_train <- training_set[trainrows,]
d_val <- training_set[-trainrows,]

unscaled_rent_amounts <- d_test$rent.amount..R.. # We use this later
mean_tr <- mean(training_set$rent.amount..R..)
std_tr <- sd(training_set$rent.amount..R..)

# Scaling
scale_data <- function(dataset, dataset2) {
  for(i in 1:ncol(dataset)){
    if (is.numeric(dataset[1,i])){
      dataset[,i] = scale(dataset[,i], center = mean(dataset2[,i]), scale = sd(dataset2[,i]))
    }
  }
  return(dataset)
} 

# Scaling
d_val <- scale_data(d_val, d_train)
d_test <- scale_data(d_test, training_set)
d_train <- scale_data(d_train, d_train) 
#We scale the training set last since we use its mean and standard deviation to scale the validation set.
```

Lastly, we encode categorical variables and keep an un-encoded version of the training and validation sets for later.

```{r,warning=FALSE,echo=FALSE}
d_train_unenc <- d_train
d_val_unenc <- d_val

encode <- function(dataset, excluded=c()) {
  excluded_cols <- dataset[, names(dataset) %in% excluded]
  dataset <- dataset[,!names(dataset) %in% excluded]
  dmy <- dummyVars(" ~ .", data = dataset)
  dataset <- data.frame(predict(dmy, newdata = dataset))
  dataset <- cbind(dataset, excluded_cols)
  return(dataset)
}  

# Encoding the categorical variables in the training, validation and test sets
d_train <- encode(d_train, c("animal", "furniture"))
d_val <- encode(d_val, c("animal", "furniture"))
d_test <- encode(d_test, c("animal", "furniture"))

# Presence of a house in a city would be represented by 0s in the other four city columns, so we can remove one city
d_train <- subset(d_train, select = -c(city.Campinas))
d_val <- subset(d_val, select = -c(city.Campinas))
d_test <- subset(d_test, select = -c(city.Campinas))
```

### Constructing Models and Performance Assessment

*AIC-BIC (Linear Model)*

We implement AIC and BIC stepwise selection for multiple regression models and evaluate the performance on the validation set using **MSE**, **RMSE** and **R Squared**.

```{r,warning=FALSE,echo=FALSE}
full.model <- lm(rent.amount..R.. ~ ., data = d_train)
step.model.aic <- stepAIC(full.model, direction = "both", trace = 0)

step.model.bic <- stepAIC(full.model, direction = "both", trace = 0, k = log1p(nrow(d_train)))

predictions.aic <- predict(step.model.aic, newdata = d_val)
predictions.bic <- predict(step.model.bic, newdata = d_val)


aic_mse <- mean((predictions.aic - d_val$rent.amount..R..)^2)
bic_mse <- mean((predictions.bic - d_val$rent.amount..R..)^2)
aic_rmse <- sqrt(aic_mse)
bic_rmse <- sqrt(bic_mse)
aic_rsquared <- summary(step.model.aic)$r.squared
bic_rsquared <- summary(step.model.bic)$r.squared

table <- data.frame(
  Model = character(),  
  MSE = numeric(),
  RMSE = numeric(),
  R_squared = numeric()
)
table <- rbind(table, c("AIC", round(aic_mse, 5), round(aic_rmse, 5), round(aic_rsquared, 5)))
table <- rbind(table, c("BIC", round(bic_mse, 5), round(bic_rmse, 5), round(bic_rsquared, 5)))
colnames(table) <- c("Model", "MSE", "RMSE", "R_squared")

table
```

The two display similar results but BIC slightly better, so we're gonna prefer it over AIC. We can also plot the residuals against the fitted values:

```{r fig4, fig.height = 2, fig.width = 6, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
aicplot <- ggplot(data.frame(Fitted = step.model.aic$fitted.values, Residuals = step.model.aic$residuals), aes(x = Fitted, y = Residuals)) +
  geom_point(color = "lightblue") + 
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "AIC", x = "Fitted Values", y = "Residuals")
bicplot <- ggplot(data.frame(Fitted = step.model.bic$fitted.values, Residuals = step.model.bic$residuals), aes(x = Fitted, y = Residuals)) +
  geom_point(color = "lightblue") + 
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "BIC", x = "Fitted Values", y = "Residuals")
ggarrange(aicplot,bicplot,nrow = 1,ncol = 2)
```

The fact that we get a similar result for AIC and BIC is a good indication of balance between model complexity and goodness of fit.

*Lasso and Group-Lasso (Penalised Approach)*

We implement a lasso regression using glmnet and group-Lasso using gglasso. 

```{r,warning=FALSE,echo=FALSE}
# Performing 10-fold cross validation
X = model.matrix(rent.amount..R.. ~ ., data=d_train)[,-1]
y = d_train$rent.amount..R..
cvlasso = cv.glmnet(x = X, y = y,nfolds = 10)
groups <- c(1,1,1,1,2,3,4,5,6,7,8,9,10,11)
glasso = cv.gglasso(x=X, y=y, group = groups,nfolds = 10)
```

```{r,warning=FALSE,echo=FALSE}
# Model Performance

pen_val <- model.matrix(rent.amount..R.. ~ ., data=d_val)[,-1]
lassopredmin <- predict(cvlasso,pen_val, s = "lambda.min")
lmin_mse <- mean((lassopredmin - d_val$rent.amount..R..)^2)
lmin_rmse <- sqrt(lmin_mse)
lmin_Rsq <- cor(lassopredmin,d_val$rent.amount..R..)^2
lassopred1se <- predict(cvlasso,pen_val, s = "lambda.1se")
lse_mse <- mean((lassopred1se - d_val$rent.amount..R..)^2)

glpred1 <- predict(glasso,pen_val, s="lambda.1se")
gl1_MSE <- mean((glpred1 - d_val$rent.amount..R..)^2)
gl1_Rsq <- cor(glpred1,d_val$rent.amount..R..)^2
glpred2 <- predict(glasso,pen_val, s="lambda.min")
gl2_MSE <- mean((glpred2 - d_val$rent.amount..R..)^2)
gl2_Rsq <- cor(glpred2,d_val$rent.amount..R..)^2

```

Let's compare the two models using minimum lambd since lambda.min values get a lower error on the validation set.

```{r,warning=FALSE,echo=FALSE}
table <- data.frame(
  Model = character(),  
  MSE = numeric(),
  RMSE = numeric(),
  R_squared = numeric()
)
table <- rbind(table, c("Lasso", round(lmin_mse, 5), round(sqrt(lmin_mse), 5), round(lmin_Rsq, 5)))
table <- rbind(table, c("grLasso", round(gl2_MSE, 5), round(sqrt(gl2_MSE), 5), round(gl2_Rsq, 5)))
colnames(table) <- c("Model", "MSE", "RMSE", "R_squared")
table
```
Once again, we notice that results are similar, however, Lasso has slightly lower errors and a higher R Squared value.

*GAM (Non-linear Model)*

We initially prepare the smoothing terms for both numerical and categorical variables in order to put up a GAM. In this stage, the training and validation sets that we previously conserved are used in their original, non-encoded forms. Next, we combine these smoothing terms to create the GAM formula. We then utilise this method to fit the model, which enables us to handle nonlinear relationships and variable interactions while efficiently analysing the data.

```{r,warning=FALSE,echo=FALSE}
num_names = names(d_train_unenc)[d_train_unenc %>% map_lgl(is.numeric)]
num_names = num_names %>% 
  discard(~.x %in% c("rent.amount..R.."))
num_feat = num_names %>% 
  map_chr(~paste0("s(", .x, ", 10)")) %>%
  paste(collapse = "+")

cat_feat = names(d_train_unenc)[d_train_unenc %>% map_lgl(is.factor)] %>% 
  paste(collapse = "+")
```

```{r,warning=FALSE,echo=FALSE}
gam_form = as.formula(paste0("rent.amount..R.. ~", num_feat, "+", cat_feat)) # formula
fit_gam = gam(formula = gam_form, family = "gaussian", data = d_train_unenc)
```

```{r fig5, fig.height = 2, fig.width = 6, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE }
ggplot(data.frame(Fitted = fit_gam$fitted.values, Residuals = fit_gam$residuals), aes(x = Fitted, y = Residuals)) +
  geom_point(color = "lightblue") + 
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted", x = "Fitted Values", y = "Residuals")
```
In this plot, the residuals predominantly cluster around the zero line, with most falling within the \([-0.5, 0.5]\) range, suggesting a generally strong fit of the model. Next, we will apply the model to the validation dataset and assess its performance using the designated metrics. 

```{r,warning=FALSE,echo=FALSE}
predicted_values = predict(fit_gam, d_val_unenc) 
observed_values = d_val_unenc$rent.amount..R..
gam_mse = mean((predicted_values - observed_values)^2)
gam_rmse = sqrt(gam_mse)
gam_rsquared = cor(predicted_values, observed_values)^2
```

*XGBoost*

For XGBoost, we separate the dependent and independent variables. Following this, we convert the training and validation datasets into xgb.DMatrix format. 

```{r,warning=FALSE,echo=FALSE}
X_train <- d_train[, !(colnames(d_train) %in% c("rent.amount..R.."))]
y_train <- as.numeric(d_train$rent.amount..R..)

X_val <- d_val[, !(colnames(d_val) %in% c("rent.amount..R.."))]
y_val <- as.numeric(d_val$rent.amount..R..)

for(i in 1:ncol(X_train)){
  X_train[,i] = as.numeric(X_train[,i])
}

for(i in 1:ncol(X_val)){
  X_val[,i] = as.numeric(X_val[,i])
}

xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_val <- xgb.DMatrix(data = as.matrix(X_val))
```

We tune the parameters `nrounds` (the number of boosting rounds), `max_depth` (the maximum tree depth) and `eta` (learning rate). We use 10-fold cross validation to do so.

```{r,warning=FALSE,echo=FALSE}
param_grid <- expand.grid(
  nrounds = c(100, 200, 300),
  max_depth = c(3, 4, 5),             
  eta = c(0.01, 0.05, 0.1),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,  
  subsample = 1   
)

# Inputting the parameter grid for 10-fold cross-validation to get the optimal combination of values for the parameters
xgb_model <- train(
  X_train, y_train, 
  method = "xgbTree",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = param_grid,
  metric = 'RMSE',
  verbosity = 0
)

#NOTE: The other parameters are included and given default values so that they have no influence of the cross-validation (not including them generated an error)
```

Now, we use the optimal parameters to fit the finalised XGBoost model and make and assess predictions on the validation set.

```{r,warning=FALSE,echo=FALSE}
final_model <- xgboost(
  data = xgb_train, 
  nrounds = xgb_model$bestTune$nrounds,
  max_depth = xgb_model$bestTune$max_depth,
  eta = xgb_model$bestTune$eta,
  verbose = 0
)

# evaluating...
predictions <- predict(final_model, newdata = xgb_val)
xgb_mse <- mean((predictions - y_val)^2)
xgb_rmse <- sqrt(xgb_mse)
xgb_rsquared <- 1 - (sum((y_val - predictions)^2) / sum((y_val - mean(y_val))^2))
```

### Model Selection

```{r,warning=FALSE,echo=FALSE}
table <- data.frame(
  Model = character(),  
  MSE = numeric(),
  RMSE = numeric(),
  R_squared = numeric()
)

table <- rbind(table, c("BIC", round(bic_mse, 5), round(bic_rmse, 5), round(bic_rsquared, 5)))
table <- rbind(table, c("LASSO", round(lmin_mse, 5), round(lmin_rmse, 5), round(lmin_Rsq, 5)))
table <- rbind(table, c("GAM", round(gam_mse, 5), round(gam_rmse, 5), round(gam_rsquared, 5)))
table <- rbind(table, c("XGBoost", round(xgb_mse, 5), round(xgb_rmse, 5), round(xgb_rsquared, 5)))
colnames(table) <- c("Model", "MSE", "RMSE", "R_squared")

table
```

The above table helps us look at the performance of each model. We notice that *XGBoost* is best performing model and we hence, select it for our ultimate test set.

### Model Implementation

We create the training set, by combining the initial training and validation set. We also scale and encode the categorial variables. 

```{r,warning=FALSE,echo=FALSE}
training_set <- scale_data(training_set, training_set)
training_set <- encode(training_set, c("animal", "furniture"))
training_set <- subset(training_set, select = -c(city.Campinas))

X_train <- training_set[, !(colnames(training_set) %in% c("rent.amount..R.."))]
y_train <- as.numeric(training_set$rent.amount..R..)

X_test <- d_test[, !(colnames(d_test) %in% c("rent.amount..R.."))]
y_test <- as.numeric(d_test$rent.amount..R..)

```

After doing so, we convert the training and test sets into xgb.Dmatrices to proceed with model fitting, and then predictions are made on the test set. It is important to evaluate these predictions (including for unscaled data for interpretability) and estimate a 95% confidence interval for the error, which will give an interval estimate of the possible deviation of the predicted rent prices from the true rent prices.

```{r,warning=FALSE,echo=FALSE}
# Making sure the columns are numeric
for(i in 1:ncol(X_train)){
  X_train[,i] = as.numeric(X_train[,i])
}
for(i in 1:ncol(X_val)){
  X_test[,i] = as.numeric(X_test[,i])
}

xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test))

final_model <- xgboost(
  data = xgb_train, 
  nrounds = xgb_model$bestTune$nrounds,
  max_depth = xgb_model$bestTune$max_depth,
  eta = xgb_model$bestTune$eta,
  verbose = 0
)

predictions <- predict(final_model, newdata = xgb_test)
unscaled_predictions <- (predictions * std_tr) + mean_tr

```

```{r,warning=FALSE,echo=FALSE}
# Evaluation of the performance
test_mse <- mean((predictions - y_test)^2)
test_rmse <- sqrt(xgb_mse)
test_rsquared <- 1 - (sum((y_test - predictions)^2) / sum((y_test - mean(y_test))^2))
unscaled_mse <- mean((unscaled_predictions - unscaled_rent_amounts)^2)
unscaled_rmse <- sqrt(unscaled_mse)
print(paste("MSE: ", round(test_mse, 5)))
print(paste("RMSE: ", round(test_rmse, 5), "| RMSE for unscaled predictions", round(unscaled_rmse, 5)))
print(paste("R Squared: ", round(xgb_rsquared, 5)))

prediction_errors <- unscaled_predictions - unscaled_rent_amounts

mean_error <- mean(prediction_errors)
std_error <- sd(prediction_errors)

confidence <- 0.95
z_value <- qnorm(1 - (1 - confidence) / 2)
lower_bound <- mean_error - z_value * std_error 
upper_bound <- mean_error + z_value * std_error 

cat("Confidence Interval (", confidence * 100, "%): [", lower_bound, ", ", upper_bound, "]\n")
```
Our analysis demonstrates that the model consistently delivers robust performance on unseen datasets, as evidenced by the low error rates on the test set, which are comparable to those on the validation set. The confidence intervals calculated offer a prediction of the true errors associated with forecasting rent prices in Brazilian reais. Specifically, the predicted rent prices could deviate by up to **654.36 Real** below or **630.64 Real** above the actual rent prices, which is a favorable outcome considering the diversity of rental prices we are analyzing.

This level of precision in predicting rent prices would allow our company to make well-informed decisions about potential rental property investments in Brazil's major urban areas. By understanding the probable returns on these investments through accurate rent price predictions, we can strategically recommed investment in properties that promise higher returns, focusing on those with lower acquisition costs but high rental potential.

Moreover, the model's high R-squared value confirms that it effectively captures a significant portion of the variance in rental prices through the variables used. This indicates a robust relationship between how the chosen independent variables help predict rent prices, affirming the reliability of our model in predicting market behaviors based on these factors. Let's proceed with seeing what is the individual impact of these independent variables. 

```{r fig6, fig.height = 3, fig.width = 6, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}

importance_scores <- xgb.importance(
  feature_names = colnames(X_train),
  model = final_model
)

importance_data <- as.data.frame(importance_scores)
importance_data$Feature <- factor(importance_data$Feature, levels = importance_data$Feature[order(-importance_data$Gain)])


ggplot(importance_data, aes(x = Feature, y = Gain, fill = Gain)) +
  geom_col(show.legend = FALSE) +  
  coord_flip() +  
  scale_fill_gradient(low = "lightblue", high = "darkblue") +  
  labs(
    title = "Feature Importance for Rental Price Prediction",
    x = "Importance",
    y = "Features"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),  
    axis.title = element_text(size = 12),  # Axis titles
    axis.text = element_text(size = 10)    # Axis text
  )
#NOTE: The term "gain" is used to indicate how much a particular feature contributes to the model's predictive power, with higher values indicating a more significant contribution.
```
The feature importance plot clearly highlights that the variable `fire.insurance..R..` is the most influential in predicting rent prices, significantly outstripping other variables in its impact. This observation underscores the critical role of fire insurance costs in forecasting rental rates.

Additionally, the variable `city` also stands out for its importance, corroborating our earlier findings from a geospatial analysis. This analysis showed distinct variations in average rent prices across different cities, with Porto Alegre and São Paulo being particularly significant. Porto Alegre was noted for having the lowest average rents, whereas São Paulo exhibited the highest, thus underlining the substantial effect of location on rental costs.

Other notable variables include `floor` and `hoa..R..`, indicating that these factors also contribute valuable insights into rent price predictions. Interestingly, factors such as the house's area and the number of rooms, which one might typically assume to be crucial, appear to have less influence than expected. This phenomenon suggests that the location of a property might overshadow its size or number of rooms in determining rental prices. For example, renting a larger house in Porto Alegre might be more affordable than renting a smaller apartment in central São Paulo.

Given the significant role highlighted for `fire.insurance..R..`, let's explore it further using a scatter plot.

```{r fig7, fig.height = 3, fig.width = 5, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
ggplot(Data, aes(x = rent.amount..R.., y = fire.insurance..R..)) +
  geom_point(alpha = 0.6, color = "blue") + 
  labs(
    title = "Rent Amount vs Fire Insurance",
    x = "Rent Amount (R$)",
    y = "Fire Insurance (R$)"
  ) +
  theme_minimal() +  
  theme(
    plot.title = element_text(hjust = 0.5), 
    axis.title.x = element_text(face = "bold", color = "darkblue"),  
    axis.title.y = element_text(face = "bold", color = "darkblue")   
  ) +
  geom_smooth(method = "lm", color = "red", se = FALSE) 


```

The scatter plot reveals a pronounced linear relationship between fire insurance costs and rent amounts, which is not unexpected. Fire insurance premiums are frequently determined based on the property's value and related risk factors. Consequently, properties commanding higher rents are usually of greater value. These properties naturally have higher replacement costs, which in turn elevate the fire insurance premiums.

Additionally, it's important to note that the location of a property (City) can also impact insurance premiums. The presence of multiple distinct lines in the plot suggests that different cities may be represented by each line. Let's look at it.

```{r fig8, fig.height = 3, fig.width = 5, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
ggplot(Data, aes(x = rent.amount..R.., y = fire.insurance..R.., color = city)) +
  geom_point(alpha = 0.6) +  
  labs(
    title = "Rent Amount vs Fire Insurance",
    x = "Rent Amount (R$)",
    y = "Fire Insurance (R$)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.title.x = element_text(face = "bold", color = "darkblue"),
    axis.title.y = element_text(face = "bold", color = "darkblue")
  ) +
  geom_smooth(method = "lm", color = "red", se = FALSE) 
```

Upon analyzing the scatter plot, it was noted that cities with lower average rent values exhibited steeper slopes on the lines representing them. This suggests that, for properties of equal value, those in cities with lower rent averages would face higher fire insurance costs. This occurs because properties in these lower-rent cities tend to be larger in terms of area and room count, leading to higher replacement costs and, consequently, higher insurance premiums.

Moreover, the plot shows two distinct lines for each city, indicating the presence of another significant factor influencing both rent amounts and fire insurance costs. This factor appears to be related to the variables `floor` and `hoa..R..`, which were significant in our predictive model. A key determinant here is whether the property is located within a condominium.

For properties, a `floor` value of 0 could mean it is either on the ground floor of a condominium or it is a standalone house. The determining factor between these scenarios is the HOA fee. A zero HOA fee suggests the property is a standalone house, whereas a nonzero HOA fee indicates the property is part of a condominium.

To delve deeper into this relationship, we propose creating a focused plot that differentiates between houses located within condominiums and independent houses. This will allow us to better understand how these factors interact in specific urban contexts.

```{r fig9, fig.height = 3, fig.width = 5, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
hoa_zero_data <- subset(Data,  Data$hoa..R.. == 0 & Data$floor == 0)
hoa_nonzero_data <- subset(Data, Data$hoa..R.. != 0)
hoa_data <- rbind(transform(hoa_zero_data, Group = "Independent"),
                  transform(hoa_nonzero_data, Group = "Condominium"))

ggplot(hoa_data, aes(x = rent.amount..R.., y = fire.insurance..R.., color = Group)) +
  geom_point(alpha = 0.6, size = 3) +  
  labs(
    title = "Rent Amount vs. Fire Insurance Costs",
    x = "Rent Amount (R$)",
    y = "Fire Insurance Cost (R$)"
  ) +
  theme_minimal() +  
  theme(
    plot.title = element_text(hjust = 0.5),  
    plot.subtitle = element_text(hjust = 0.5),  
    axis.title.x = element_text(color = "darkblue"),  
    axis.title.y = element_text(color = "darkblue"),  
    legend.title = element_text(size = 12), 
    legend.text = element_text(size = 10)  
  ) +
  scale_color_manual(values = c("Independent" = "blue", "Condominium" = "green")) +  # Manually setting colors for groups
  geom_smooth(method = "lm", se = FALSE, aes(group = Group), color = "black")  # Adding linear model lines for each group

```
The scatter plot clearly supports our hypothesis, indicating that independent house renters incur higher fire insurance costs than apartment renters in condominiums, even at equivalent rent levels. This discrepancy may stem from the absence of shared fire safety features in independent houses that are typically present in condominiums. 

Such features in apartments help mitigate fire risks and, consequently, lower insurance premiums. Furthermore, independent houses often require additional insurance coverage for separate structures like garages, contributing to the increased premium costs.

### Drawing Conclusions - Task 1

To summarise the conclusions from our analysis, a strategic investment approach that encompasses both independent houses and condominium apartments could effectively cater to a broad spectrum of rental market demands. This strategy enables the company to meet the varying preferences and priorities of different renters. For instance, those who value privacy, ample space, and freedom may prefer renting independent houses, despite potentially higher rent and fire insurance costs and locations further from urban centers. Conversely, renters seeking affordability may be drawn to apartments within condominiums.

The company ought to conduct a thorough evaluation of potential investment returns, considering factors such as location, initial investment costs, associated risks, and available capital. Our predictive model, which reliably estimates rent prices, proves to be an indispensable tool in this analytical process, aiding the company in making informed investment decisions.


## Task 2

The objective of our second task is to *Cluster the houses for rental according to their characteristics*. We use K-Means and Hierarchical Clustering to do so and select the optimal number of k using the silhouette method.

```{r}
# removing categoral variables
Data_sc <- Data %>% select_if(is.numeric)
Data_sc <- as.data.frame(scale(Data_sc))

dist_p <- factoextra::get_dist(Data_sc,method = "pearson")
dist_e <- factoextra::get_dist(Data_sc,method = "euclidean")

set.seed(444)
```


```{r,warning=FALSE,echo=FALSE}
sil_k <- fviz_nbclust(
  Data_sc,
  FUNcluster = kmeans,
  diss = dist_e,
  method = "silhouette",
  print.summary = TRUE,
  k.max = 10
)
sil_h <-fviz_nbclust(
  Data_sc,
  FUNcluster = factoextra::hcut,
  diss = dist_e,
  method = "silhouette",
  print.summary = TRUE,
  k.max = 10
)
print(paste("Best k for K-Means:",which(sil_k$data$y == max(sil_k$data$y)),",Best k for for Hierachial Clustering:",which(sil_h$data$y == max(sil_h$data$y))))
```
We visualise the partitions for both methods: K-Means and Hierarchial Clustering.

```{r fig10, fig.height = 3, fig.width = 5, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
km = kmeans(Data_sc, 2, nstart = 1, iter.max = 1e2)
kmv <- fviz_cluster(km, data = Data_sc, geom = "point", 
             ggtheme = theme_minimal(), main = "K-Means")
kmv
```
```{r fig114, fig.height = 3, fig.width = 5, fig.align = "center", echo=FALSE,warning=FALSE,message=FALSE}
hcc <- factoextra::hcut(x = dist_e, 
                         k = 4,
                         hc_method = "ward.D2")
ec <-factoextra::fviz_cluster(list(data = Data_sc, cluster = hcc$cluster), main = "Hierarchical",labelsize = 0)

ec 
```

```{r,warning=FALSE,echo=FALSE}
sk = silhouette(km$cluster, 
                 dist = dist_e)
skv <- fviz_silhouette(sk,print.summary = FALSE)
print(paste("K-Means Average Silhouette width:",mean(skv$data$sil_width)))


hcc <- factoextra::hcut(x = dist_e, 
                         k = 4,
                         hc_method = "ward.D2")
print(paste("Hierarchical Clustering Average Silhouette width:",hcc$silinfo$avg.width))
```

We notice that with hierarchial clustering we got some issues. As we can see above, the number of K is 4 which resulted in overlapping and further, it was also difficult to draw a direct conclusion on the basis of characteristics that determine what cluster a data point belongs to. Further, K-Means even has a higher average silhouette width and for these reasons, we choose **K-Means as our choice of clustering** method and base our further analysis on the same. This clustering is valid as we can assume it to base clusters to determine two groups: affordable housing and expensive housing. Let's explore these clusters further.

```{r fig11, fig.height = 2, fig.width = 8, fig.align = "center",,warning=FALSE,message=FALSE,echo=FALSE}
clust1 <- Data[which(km$cluster == 1),]
clust2 <- Data[which(km$cluster == 2),]

cluster_stats <- data.frame(
  Cluster = c("Cluster 1", "Cluster 2"),
  Avg_Rent_Amount = c(round(mean(clust1$rent.amount..R..), 3), round(mean(clust2$rent.amount..R..), 3)),
  Avg_Property_Tax = c(round(mean(clust1$property.tax..R..), 3), round(mean(clust2$property.tax..R..), 3)),
  Avg_Rent_Per_Room = c(round(mean(clust1$rent.amount..R../clust1$rooms), 3), round(mean(clust2$rent.amount..R../clust2$rooms), 3))
)

cluster_stats
```

We make the very interesting observation that the average rent amount in cluster 2 is significantly higher than that of cluster 1 which as priorly mentioned, signifiess lower and higher end housing. We notice the same for average property tax and average rent per room where cluster 2 is prominently more 'expensive' than cluster 1. Let's lastly visualise if this aligns with our very first visualisation of certain cities being more expensive than others.

```{r,warning=FALSE,echo=FALSE}
display_city_pie_chart <- function(Data, data_name){  
  
  city_df <- as.data.frame(table(Data$city))
  colnames(city_df) <- c("city", "count")
  
  city_df$percentage <- city_df$count / sum(city_df$count)
  
  ggplot(city_df, aes(x = "", y = percentage, fill = city)) +
    geom_bar(width = 1, stat = "identity") +
    coord_polar("y", start = 0) + 
    scale_fill_brewer(palette = "Set3") +
    theme_void() +
    labs(title = paste("Pie Chart of Cities in", data_name), fill = "City")
}

pie1 <- display_city_pie_chart(clust1,"Cluster1")
pie2 <- display_city_pie_chart(clust2,"Cluster2")
ggarrange(pie1,pie2,nrow = 1,ncol = 2)
```

### Drawing Conclusions - Task 2

Analyzing the data reveals distinct clusters representing different segments of the housing market in the region of Brazil under study, with a significant concentration of properties in Sao Paulo, reflecting both high and low-end housing options. The observed patterns indicate a correlation between property tax, rent per room, and overall rent prices within the identified clusters.

Considering business objectives, if the company seeks to expand its real estate holdings with a focus on *affordability*, targeting properties within *cluster 1* would be advantageous. These properties likely offer lower acquisition costs, enabling the company to scale its portfolio while maintaining competitive rental rates, appealing to budget-conscious renters.

Alternatively, if the company aims to emphasize premium properties with *higher rental potential*, prioritizing acquisitions within *cluster 2* is recommended. Despite potentially higher acquisition costs, these properties are positioned to yield greater rental income, aligning with a strategy geared towards luxury housing markets.

For portfolio diversification, leveraging insights from both clusters facilitates a balanced approach. By considering properties across a spectrum of cost ranges, the company can broaden its rental offerings, catering to diverse tenant preferences and optimizing investment returns.
